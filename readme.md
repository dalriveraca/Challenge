

##  Data Engineering Challenge: Ingesta y Procesamiento Transaccional

-----

##  Objetivo del Challenge

Este repositorio contiene la soluci贸n completa para el Challenge T茅cnico de Ingenier铆a de Datos, cuyo objetivo principal fue dise帽ar e implementar un **pipeline ETL** robusto y optimizado para **ingestar datos transaccionales (JSONL anidados)** y prepararlos para el consumo anal铆tico en un entorno de **SQL Server**.

La soluci贸n fue dise帽ada priorizando la **eficiencia de memoria** en el procesamiento Python y la **flexibilidad** en el consumo de datos v铆a SQL.

-----

##  Arquitectura del Pipeline

El flujo de trabajo se estructura en un dise帽o modular de tres etapas principales ejecutadas en Python, garantizando un control de fallos granular y una alta optimizaci贸n de recursos.

### 1\.  An谩lisis y Preparaci贸n de Datos

  * **Exploraci贸n Inicial:** Se realiz贸 un an谩lisis exploratorio en Python para determinar el formato, identificar la data **transaccional** (sin claves primarias) y localizar **datos faltantes** en las estructuras JSONL remitidas.
  * **Decisi贸n Tecnol贸gica:** Se descart贸 el uso de integraciones nativas de GCP/BigQuery (que aplanan JSONL autom谩ticamente) para demostrar la capacidad de realizar el **aplanamiento de archivos JSONL** 铆ntegramente mediante c贸digo Python.

### 2\. 锔 Parametrizaci贸n y Modelado en SQL Server

Se dise帽贸 la base de datos para funcionar como un **repositorio de datos inmutable**, apto para el consumo anal铆tico sin riesgo de modificaci贸n:

  * **Creaci贸n de Tablas:** Se crearon tres tablas maestras en SQL Server que reflejan la informaci贸n transaccional.
  * **Integridad de Datos:** Se procedi贸 a **crear manualmente llaves primarias** y configurar los tipos de datos en la base de datos de destino para garantizar la integridad y el buen rendimiento de las consultas.

### 3\.  Ingesta Eficiente de la Informaci贸n

La fase de carga se ejecut贸 con una soluci贸n Python optimizada para el volumen de datos manejado (informaci贸n transaccional de un mes):

  * **Herramientas:** Uso de **`json`**, **`pandas`** y **`pyodbc`**.
  * **Procesamiento:** Se realiz贸 el aplanamiento de los archivos JSONL, el ajuste de nombres de columnas y la coerci贸n de tipos de datos (**`astype()`**) para la eficiencia de memoria (ej. `float64` a `float32`).
  * **Carga Masiva:** Se utiliz贸 **`pyodbc`** con el m茅todo **`executemany`** para la carga de datos a SQL Server, garantizando un proceso de inserci贸n masiva y r谩pido.
  * **Escalabilidad Futura:** Se justifica el uso de **Pandas** por el tama帽o manejable de la data, pero se deja clara la alternativa de migrar a **PySpark** en caso de enfrentar vol煤menes de datos mayores a los 5GB.

-----

##  Procesamiento y Consumo SQL

El enfoque del procesamiento fue facilitar la sinergia entre los equipos de desarrollo y an谩lisis:

  * **L贸gica en SQL:** El procesamiento final (transformaci贸n y agregaci贸n) se realiza con c贸digo SQL ejecutado directamente en el servidor.
  * **Parametrizaci贸n:** Las consultas SQL est谩n dise帽adas para trabajar con **par谩metros embebidos** en el c贸digo. Esto permite modificar la l贸gica de consulta (ej. rangos de fechas, filtros de estado) sin necesidad de modificar el c贸digo principal de Python, reduciendo riesgos de fallos de integraci贸n.

-----

##  Notas de Dise帽o y Optimizaci贸n

El proyecto se estructura en tres secciones separadas para garantizar la calidad y la optimizaci贸n:

  * **Modularidad y Debugging:** Dividir el proceso en tres secciones (An谩lisis/Ingesta/Procesamiento) permite identificar r谩pidamente qu茅 parte del *pipeline* fall贸 (conexi贸n, transformaci贸n, o carga) para una **gesti贸n de correcci贸n m谩s r谩pida y concreta**.
  * **Control de Versiones:** Todo el proyecto se encuentra en un repositorio Git para el **control de versiones** y la trazabilidad del c贸digo.
  * **Optimizaci贸n de Recursos:** Se prioriz贸 el uso eficiente de la memoria y el consumo de recursos en Python (evitando la sobrecarga y los tipos de datos ineficientes) y se aplic贸 la carga masiva en SQL Server.